{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtwaugh/DQN/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mAnUCp6bQC7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Q Learning in Python\n",
        "\n",
        "Let's build a deep Q network! For a recent interview, I spent three days bootstrapping one of these to prototype a learn-to-rank system, and it felt worthwhile to revisit this and really nail down the details. I've also been meaning to give myself a real project to teach myself TensorFlow, and this task seemed as good as any. In this first bit, I build a deep Q learner following the mathematical specifications, and I run it in an OpenAI gym environment."
      ]
    },
    {
      "metadata": {
        "id": "ek9tQXYKp75X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is a Deep Q Network?\n",
        "\n",
        "Great question. Let's wow you with a diagram first, because it really feels like there ought to be an immediate visual for a complicated object:\n",
        "\n",
        "![alt text](https://imgur.com/rrXj8vI.png)\n",
        "\n",
        "Cool. The intuitive explanation is pretty simple. We have an actor who interacts with an environment, which has a state. Acting on the environment changes its state and elicits a certain reward. The goal is to maximize reward by modifying behavior. In Q learning, the program keeps track of its history, and the actor accesses the history in order to train on past events. At every step, the actor berates himself for past actions, estimating in retrospect how much reward he thinks he could have elicited in comparison to what he got, and readjusts. I find this a relatable human phenomenon, and I suspect you do, as well. \n",
        "\n",
        "In order to do this learning, the actor needs to keep track of both his present method and the evolving method that updates through the training process. Imagine trying to learn a new skill and being told by your mentor to \"do that again the way you were doing it, so we can see what you'e doing wrong.\" Same deal for the actor: in order to avoid comparing an updating function against itself, we need a network to hold a frozen copy of the actor's parameters during training.\n",
        "\n",
        "Anyway, I've separated the arrows into four types. The gold arrows indicate encapsulated behavior. In the networks, this is just TensorFlow doing neural net stuff; in the environment, this is totally unseen by the program written here and we won't pretend to know anything about it. The solid arrows represent the flow of information during the interaction process: the actor sees a state, acts, and it changes the environment and produces reward. The dashed arrows are simply the recording of historical data. The dotted arrows represent the flow of information during the learning process: the actor replays the history, computing best possible actions according to the frozen estimator, and adjusting the new parameters to gradient-descend toward the maxima from the frozen ones. Finally, the frozen parameters have to slowly update toward the new ones to keep things moving."
      ]
    },
    {
      "metadata": {
        "id": "L33lcjW1gJP_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "e4933c23-5234-47bc-8db8-cb8e4a688a98"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install swig"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 2 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 5s (214 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 22280 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WuA1LTrwYsd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4fc96559-2000-4f01-81ff-5c20b4d04236"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install gym\n",
        "!pip3 install Box2D\n",
        "!pip3 install box2d box2d-kengz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/44/3a63e8b87f642db49ac81239620e68df8cfae223dcfda4f8508aec88d204/gym-0.10.8.tar.gz (1.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 15.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (0.19.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Collecting pyglet>=1.2.0 (from gym)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 17.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Running setup.py bdist_wheel for gym ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/ec/dd/33bcc8801d345f0b640fced8a0864a7c8474828564bc5ccf70\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "Successfully installed gym-0.10.8 pyglet-1.3.2\n",
            "Collecting Box2D\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/7b/ddb96fea1fa5b24f8929714ef483f64c33e9649e7aae066e5f5023ea426a/Box2D-2.3.2.tar.gz (427kB)\n",
            "\u001b[K    100% |████████████████████████████████| 430kB 17.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Box2D\n",
            "  Running setup.py bdist_wheel for Box2D ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/35/09/fd/054e73da7184a08071ed889bf45772719c7bb6d2dd13f166a1\n",
            "Successfully built Box2D\n",
            "Installing collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.2\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Collecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K    100% |████████████████████████████████| 430kB 20.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-kengz\n",
            "  Running setup.py bdist_wheel for box2d-kengz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d-kengz\n",
            "Successfully installed box2d-kengz-2.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "od94BwgaZZ8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the relevant imports:"
      ]
    },
    {
      "metadata": {
        "id": "DedwQDZaY0Hn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugBCvYs2ZjmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Okay, But I Really Just Wanted to Know Why It's Called a 'Q' Network...\n",
        "\n",
        "Sure. If you've done any advanced math, these sorts of names come up all the time (algebraic K theory, for instance), and you ought to know better than to ask. But if you're like me, then you still want to ask, and this time there actually is a good answer: Q is the name of the hidden function that returns the maximum reward for each action at each state, and the actor is trying to lern it. \n",
        "\n",
        "Let's use this as a segue into some notation.\n",
        "\n",
        "Because we're doing reinforcement learning, our session consists of an \"environment\" object (in this case, the OpenAI gym) and an \"actor\" object represented by a neural network, and the actor will choose actions based on the observed state of the environment weighted by the speculative reward that they will produce. The map from states to actions we call the actor's *policy*. Because the network is parameterized, we can use stochastic gradient descent to try to find an optimal policy for the environment, which we take to be a satisfactory solution to the problem.\n",
        "\n",
        "When we speak of value, of course, it's important to remember that we are in fact trying to choose a policy that creates the most *long-term* value, i.e. not naive greedy value. This matters for understanding deep Q learning. Let $R_t : s_t, a_t \\mapsto r_t$ denote the reward function of the environment at time $t$. As I mentioned, we assume no conditions on the behavior of $R$. Additionally, let $\\pi: s_t \\mapsto a_t$ denote the actor's policy. Because it's parameterized by a neural net, we can speak of a maximum over functions indexed over it. Finally, let $\\gamma \\in [0..1]$ be the *discount rate*, which determines the trade-off between present and future value. Now recall the Bellman equation from the machine learning literature:\n",
        "\n",
        "$$ Q(s_0) = \\max_{\\pi} \\sum_{t = 0}^{\\infty} \\gamma^t r_t$$\n",
        "\n",
        "This states that the desirability of the environment being in some state $s_0$ is the maximum long-term reward it could yield under any policy. By \"long-term reward\" we mean the sum of reward values (remember $r_t = R_t(s_t, a_t)$ and $s_t$ is a (possibly nondeterministic) function of $a_{t-1}$ and $s_{t-1}$, and $a_t = \\pi(s_{t-1})$.\n",
        "\n",
        "Some pseudocode is probably instructive at this point. "
      ]
    },
    {
      "metadata": {
        "id": "qUf_iSLdqU70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General sketch of a policy iteration\n",
        "# Additional code will try to improve the function actor.policy()\n",
        "def pseudo_policy_iterate(env, actor, maxTime):\n",
        "  actor.initialize()\n",
        "  env.initialize()\n",
        "  # Get s_0\n",
        "  state = env.get_initial_state()\n",
        "  \n",
        "  for t in range(maxTime):\n",
        "    # a_t = \\pi(s_{t-1})\n",
        "    action = actor.policy(state)\n",
        "    # r_t = R_t(s, a)\n",
        "    reward = env.get_reward(t, state, action)\n",
        "    # a_{t-1}, s-{t-1} ->  s_t\n",
        "    new_state = env.reaction(state, action)\n",
        "    state = new_state\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2MrsOAMr-HI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the function $Q$ is known completely, and if the state resulting from each action is deterministically known, then a greedy algorithm directly yields the optimal policy, i.e. the information about the optimal policy is already in the function. However, we know neither the full reward function nor do we know how to reach one state from another by means of a certain action. As it turns out, a greedy algorithm will remain optimal in most practical cases, but this is merely an empirical fact. In light of this, we really want to investigate the following:\n",
        "\n",
        "$$ Q_{\\pi}(s, a) = \\mathbb{E}\\left[ \\sum_{t = 0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a, a_t = \\pi(s_{t-1}) \\right] $$\n",
        "\n",
        "That is, we suppose we have partial information about the best policy given an action on a certain present state and we set our actor to work trying to estimate a policy that approaches the value $Q_*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$ at each state $s$ and each action $a$.\n",
        "\n",
        "There you have it: it's a deep neural net that's trying to efficiently approximate $Q_*$ in order to determine an optimal policy in a given environment."
      ]
    },
    {
      "metadata": {
        "id": "PTwKd7W3ATOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Loss Function with Respect to Historical Data\n",
        "\n",
        "Great, so how do we define a loss function that gets us there? Obviously there's no guarantee of a global maximum, so we have to choose some sort of compromise that will converge us to a good estimate with reasonable resources. \n",
        "\n",
        "What's distinctive about deep Q learning is that the actor \"introspects\" after every action: the program keeps a \"subject history\" of the actor, which will get minibatch-sampled at every step to readjust the actor network against a new estimate of the maximum-value policy. The idea here is to continually re-estimate our long-term rewards given new information from the environment. That is, we want to figure out the best the actor \"could have done\" given all of its present information, then compare the current policy to the estimated ideal performance. Let $\\hat{Q}(s, a, \\theta)$ denote the actor network's estimate of $Q(s, a)$ at parameters $\\theta$. We define the *time-difference (TD) error* as follows:\n",
        "\n",
        "$$\\delta(t) = | r_t + \\gamma \\cdot \\max_{a_{t+1}} \\hat{Q}(s_{t+1}, a_{t+1}, \\theta^{-})  - \\hat{Q}(s, a, \\theta)  | $$\n",
        "\n",
        "Here we denote by $\\theta^{-}$ the set of parameters the neural net takes at the beginning of the experience-replay process, whereas $\\theta$ is allowed to update continuously over a replay-training session. Again, we'll pseudocode this:"
      ]
    },
    {
      "metadata": {
        "id": "_fEAqU3ss1NW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pseudo_replay_learn(actor, history, gamma):\n",
        "  # Freeze our estimator parameters\n",
        "  target_params = copy(actor.parameters)\n",
        "  # Learn over many iterations\n",
        "  for epsiode in range(max_episodes):\n",
        "    loss = 0.0\n",
        "    for event in history:\n",
        "      (state, action, reward, time, newState) = event\n",
        "      # Compute \\hat{Q}(s_{t+1}, a_{t+1}, \\theta^-)\n",
        "      action_values = [ actor.estimate(newState, newAction, target_params)\\\n",
        "                        for newAction in actor.actions ]\n",
        "      # Get our new estimate\n",
        "      new_estimate = actor.estimate(state, action, actor.parameters)\n",
        "      # Get our TD error\n",
        "      delta = (reward + gamma * max(action_values) - new_estimate)\n",
        "      # Update our MSE\n",
        "      loss += delta**2\n",
        "    # Adjust our estimate weights each episode\n",
        "    actor.update_weights(error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6dTxSRnZvreb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " This will fit into our pseudocode as follows:"
      ]
    },
    {
      "metadata": {
        "id": "HpVynI00xUxu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General sketch of a policy iteration\n",
        "# Additional code will try to improve the function actor.policy()\n",
        "def pseudo_train(env, actor, gamma, max_time):\n",
        "  # Start the history\n",
        "  history = []\n",
        "  # Start the black boxes\n",
        "  actor.initialize()\n",
        "  env.initialize()\n",
        "  # Get s_0\n",
        "  state = env.get_initial_state()\n",
        "  \n",
        "  for t in range(max_time):\n",
        "    # a_t = \\pi(s_{t-1})\n",
        "    action = actor.policy(state)\n",
        "    # r_t = R_t(s, a)\n",
        "    reward = env.get_reward(t, state, action)\n",
        "    # a_{t-1}, s-{t-1} ->  s_t\n",
        "    new_state = env.reaction(state, action)\n",
        "    # Memorize\n",
        "    history.append((state, action, reward, time, newState))\n",
        "    # Replay the memory bank and learn\n",
        "    pseudo_replay_learn(env, actor, gamma)\n",
        "    # Advance in time\n",
        "    state = new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRtiHwwfxx7F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of course, we have to tune a number of hyperparameters here. And, of course, in reality we're not going to read the entire history every time but rather minibatch-sample it. And we probably only want to train every several time steps. And we want a max size on the history. But the effect is the same: we're trying to efficiently estimate at each step the total divergence from the best forseeable performance at that step, and then backpropagate the actor network. \n",
        "\n",
        "Following the seminal paper by Mnih et al. (2015), we can write the minibatch loss function, letting $U(D)$ denote a uniform distribution over the history as\n",
        "\n",
        "$$ \\mathcal{L}_i(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\ \\sim \\ U(D)} \\left[ \\left( r + \\max_{a'} \\hat{Q}(s', a', \\theta_i^-) - \\hat{Q}(s, a, \\theta_i) \\right)^2 \\right] $$\n",
        "\n",
        "Okay, let's first create a flag for debug printing, and then build a buffer that has a size cap, curries for us, and allows us to easily sample it. The ``append`` method maes sure I don't have to deal with tuples, and the ``sample`` method returns batches of states, actions, and rewards as numpy arrays of the appropriate shape."
      ]
    },
    {
      "metadata": {
        "id": "mXuv_JMjEzlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "debug_print = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nrtdLywp0UsS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stores tuples of (state, action, reward, time, new_state)\n",
        "class Buffer:\n",
        "\n",
        "    def __init__(self, cap=1000, seed=1728):\n",
        "        random.seed(seed)\n",
        "        self.buffer = deque()\n",
        "        self.count = 0\n",
        "        self.cap = cap\n",
        "\n",
        "    def append(self, s, a, r, t, s2):\n",
        "        unit = (s, a, r, t, s2)\n",
        "        # Make sure to respect the max capacity\n",
        "        if self.count < self.cap: \n",
        "            self.buffer.append(unit)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(unit)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Get events\n",
        "        batch = random.sample(self.buffer, min([self.count, batch_size]))\n",
        "        # Print if flagged\n",
        "        if debug_print:\n",
        "            print(\"Sampled \", batch_size, \" events from the buffer.\")\n",
        "        # Now collate the events\n",
        "        # Concatenate here because they're shape (1, state_dims)\n",
        "        s_batch = np.concatenate([s for (s, a, r, t, s2) in batch])\n",
        "        # Reshape here because we need a second dimension\n",
        "        a_batch = np.reshape([a for (s, a, r, t, s2) in batch], (batch_size, 1))\n",
        "        r_batch = np.reshape([r for (s, a, r, t, s2) in batch], (batch_size, 1))\n",
        "        t_batch = np.reshape([t for (s, a, r, t, s2) in batch], (batch_size, 1))\n",
        "        # Concatenate here because they're shape (1, state_dims)\n",
        "        s2_batch = np.concatenate([s2 for (s, a, r, t, s2) in batch])\n",
        "        return s_batch, a_batch, r_batch, t_batch, s2_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oyMStkAX00xX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll build an actual deep Q network. As alluded to above, when we freeze the parameters $\\theta^-$, we're better off just keeping an entire other network, christened the *target network*. To do this, then, we include a function to build these both. \n",
        "\n",
        "Additionally, we're going to have to custom-define the loss function with its own computation graph, since (to my knowledge) TensorFlow does not include a built-in Q-learning module. (It's also a good exercise.) This is part of the beauty of TensorFlow. Here's a sketch I made of what's going on. The gold boxes represent placeholder Tensor objects with no defined size, whereas the grey boxes represent batches of size ``batch_size``. Considering only the graph on the gold boxes allows us to define a \"flow\" for the tensors that's represented totally abstractly. Nowhere in defining the loss function do I call ``session.run()``, because the function is just a graph.\n",
        "\n",
        "![alt text](https://imgur.com/FmSDJAH.png)\n",
        " \n",
        "We can see the flow between the two networks, although the actual mathematics is now obscured. When we use the object ``self.loss``, the whole depicted apparatus of circular nodes and their edges is what we're working with. It was working through how to set this up that finally made the meaning of the term \"TensorFlow\" finally click for me.\n",
        "\n",
        "In fact, this is the only effective way to do this. If we remember how backpropagation works, the only way to actually learn is to adjust each parameter in each network according to the loss function's gradient with respect to that parameter. The only way that we can get TensorFlow to compute all those gradients for us is to define the loss function as a graph. That is, had we not done it this way, the loss function wouldn't be TensorFlow-differentiable and we'd be hosed. However, this way, training is now straightforward. Just take the optimizer that we're accustomed to using and tell it to minimize the custom loss function. I define a ``train`` function as a wrapper, but as you can see, all it does is run the provided session on the loss function mimizer."
      ]
    },
    {
      "metadata": {
        "id": "lZhAHyev1JbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor:\n",
        "\n",
        "    def __init__(self, state_dims, actions_num, learning_rate, tau):\n",
        "        # Network parameters\n",
        "        self.state_dims = state_dims\n",
        "        self.actions_num = actions_num\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = learning_rate\n",
        "        # Learning rate for target network\n",
        "        self.tau = tau\n",
        "\n",
        "        # Assemble the actor\n",
        "        self.actor_input, self.actor_network = self.build_nn()\n",
        "        self.actor_params = tf.trainable_variables()\n",
        "        # Might want to normalize these\n",
        "        self.actor_gradients = tf.gradients(self.actor_network,\\\n",
        "                                            self.actor_params)\n",
        "        # Get the optimizer involved\n",
        "        self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
        "\n",
        "        # Assemble the target network - this keeps values constant in training\n",
        "        self.target_input, self.target_network = self.build_nn()\n",
        "        self.target_params = tf.trainable_variables()[len(self.actor_params):]\n",
        "        # We might want to normalize these\n",
        "        self.target_gradients = tf.gradients(self.target_network,\\\n",
        "                                             self.target_params)\n",
        "\n",
        "        # Have the target parameters slowly catch up to the actor parameters\n",
        "        self.update_target = [ self.target_params[i].assign(\\\n",
        "            tf.multiply(self.actor_params[i], self.tau) +\n",
        "            tf.multiply(self.target_params[i], 1. - self.tau))\\\n",
        "            for i in range(len(self.target_params)) ]\n",
        "\n",
        "        # Placeholders for the batch values\n",
        "        self.loss_actions = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.loss_rewards = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        # Build the graph for the loss function\n",
        "        target_rewards = self.loss_rewards + \\\n",
        "                          tf.reduce_max(self.target_network, axis=1)\n",
        "        estimates = tf.batch_gather(self.actor_network, self.loss_actions)\n",
        "        td_error = target_rewards - estimates\n",
        "        self.loss = tf.reduce_mean(tf.square(td_error))\n",
        "        \n",
        "        # Operation for updating the actor network's weights\n",
        "        self.update_actor = self.opt.minimize(self.loss)\n",
        "        \n",
        "    # Encapsulate this because we're going to call it twice\n",
        "    def build_nn(self):\n",
        "        inputLayer = tf.placeholder(tf.float32, shape=(None, self.state_dims))\n",
        "        hiddenLayer = Dense(64)\n",
        "        # The network is trying to estimate the value for each possible action\n",
        "        outputLayer = Dense(self.actions_num)\n",
        "        # And put them all together\n",
        "        layers = BatchNormalization()(inputLayer)\n",
        "        layers = Activation('relu')(layers)\n",
        "        layers = hiddenLayer(layers)\n",
        "        layers = BatchNormalization()(layers)\n",
        "        layers = Activation('relu')(layers)\n",
        "        layers = outputLayer(layers)\n",
        "        layers = Activation('softmax')(layers)\n",
        "        # We have to return the input layer to give it data\n",
        "        return inputLayer, layers\n",
        "       \n",
        "    # Predict on a numpy array\n",
        "    # Should return an integer for the action number\n",
        "    def predict(self, session, inputs):\n",
        "        return session.run(self.actor_network,\\\n",
        "                           feed_dict={ self.actor_input : inputs } )\n",
        "\n",
        "    # Push the minibatch to the loss function minimizer\n",
        "    def train(self, session, s_batch, a_batch, r_batch, s2_batch):\n",
        "        session.run( self.update_actor,\\\n",
        "                     feed_dict = { self.actor_input : s_batch,\\\n",
        "                                   self.loss_actions : a_batch,\\\n",
        "                                   self.loss_rewards : r_batch,\\\n",
        "                                   self.target_input : s2_batch } )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nEbrpgT0PUZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Environment and Exploration/Exploitation\n",
        "\n",
        "Finally, I define the full training routine. This should bear reasonable resemblance to the pseudocode above, plus some print statements.\n",
        "\n",
        "Actually, there is one twist that bears some discussion. Notice that I include a chance of acting randomly: this is called an $\\varepsilon$*-greedy algorithm* and it's an attempted answer to the problem of *exploration-exploitation* tradeoff. This is a pretty easy concern to visualize, especially in light of the performance this code initially elicits. \n",
        "\n",
        "Here's the MountainCar environment that I deploy the network in:\n",
        "\n",
        "![alt text](http://gym.openai.com/v2018-02-21/videos/MountainCar-v0-270f34b9-f23e-4d95-a933-4c902b4f4435/poster.jpg)\n",
        "\n",
        "The environment simulates Newtonian mechanics in two dimensions, and you can see from your own physical intuition how the actor can win. The reward is -1.0 for every frame at which the cart is not sitting at the flag, the the environment terminates as soon as the cart touches the flag. Therefore, the optimal policy gets the cart to the top as quickly as possible.\n",
        "\n",
        "How will the deep Q learner find this policy? Imagine we've been sitting for one hundred frames and trying to learn on past experiences. Having not yet seen a successful policy, the learner will not be maximiing toward anything meaningful. In fact, naively learning like this prevents us from ever succeeding. What's to be done?\n",
        "\n",
        "There's a problem called the [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) and it lies in that class of problems with stupid-sounding names that you just can't aviod hearing mention of when you study the field to which they pertain. It describes essentially the predicament we're about to get into: it's clear that the actor needs to behave randomly sometimes in order to have anything other than past experience to learn on. In other words, in order to avoid local minima of $\\hat{Q}_{\\theta}$ over the parameter space, we have to *explore* it a bit. However, exploring too much will make the actor unable to actually pursue a single policy and at worst give rise to random motion with no learning. That is, it does need to keep *exploiting* the value it finds through exploration.\n",
        "\n",
        "How do we trade off? Well, the solution to the multi-armed bandit problem, which I am appropriating here without really thinking about it, is the $\\varepsilon$-greedy method. Every step, with probability $p = (1 - \\varepsilon) + \\varepsilon / |A|$ for $A$ the set of possible actions, the actor follows his intuition. Otherwise, he performs a random action."
      ]
    },
    {
      "metadata": {
        "id": "Y0KKuwr7o3Ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training routine within a TF session\n",
        "# TODO log the reward values for TensorBoard\n",
        "def train(session, env, actor,\\\n",
        "          epsilon=0.10, ep_num=10000, ep_length=100, minibatch_size=64):\n",
        "    env.reset()\n",
        "    # Keep the buffer here\n",
        "    history = Buffer()\n",
        "    # Update target network parameters\n",
        "    session.run(actor.update_target)\n",
        "    # Start running episodes\n",
        "    for ep in range(ep_num):\n",
        "        # Print\n",
        "        print(\"EPISODE \", ep)\n",
        "        # Use matplotlib to render in Colab\n",
        "        #plt.imshow(env.render(mode='rgb_array'))\n",
        "        # Per episode, reset the environment\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        ep_avg_max = 0\n",
        "        # Run through the epochs\n",
        "        for epoch in range(ep_length):\n",
        "            \n",
        "            # Get the most favored action given the actor's prediction\n",
        "            state_vec = np.reshape(state, (1, actor.state_dims))\n",
        "            \n",
        "            # Get the action as the index in the distribution\n",
        "            # But we want to do this epsilon-greedily or else get stuck\n",
        "            action = -1\n",
        "            if random.random() > (1 - epsilon) + (epsilon / actor.actions_num):\n",
        "                action = random.randint(0, actor.actions_num - 1)\n",
        "            else:\n",
        "                action = np.argmax(actor.predict(session, state_vec))\n",
        "            \n",
        "            # Print if flagged\n",
        "            if debug_print:\n",
        "                print(\"Performed action \", action)\n",
        "            \n",
        "            # Feed this into the gym environment\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            # Print if flagged\n",
        "            if debug_print:\n",
        "                print(\"Got reward \", reward)\n",
        "                print(\"Resulted in state \", new_state)\n",
        "            \n",
        "            # If the actor won or died or whatever it is, break the loop\n",
        "            if done:\n",
        "                # Print if flagged\n",
        "                if debug_print:\n",
        "                    print(\"Terminated.\")\n",
        "                break\n",
        "                \n",
        "            # Store the results as a tuple\n",
        "            history.append( state_vec, action, reward, done, \\\n",
        "                            np.reshape(new_state, (1, actor.state_dims)) )\n",
        "            \n",
        "            # If we're large enough, take a minibatch sample and train the net\n",
        "            if history.count > minibatch_size:\n",
        "                # Get the arrays corresponding to the values\n",
        "                s_batch, a_batch, r_batch, t_batch, s2_batch =\\\n",
        "                 history.sample(minibatch_size)\n",
        "                # Feed this to the DQN to train\n",
        "                actor.train(session, s_batch, a_batch, r_batch, s2_batch)\n",
        "            \n",
        "            # Update the state\n",
        "            state = new_state\n",
        "            # Tabulate reward\n",
        "            ep_reward += reward\n",
        "            # The paper claims we do this in here\n",
        "            # Let the target parameters crawl toward the actor parameters\n",
        "            session.run(actor.update_target)\n",
        "        # Print\n",
        "        print(\"Total reward: \", ep_reward, \"\\n-----------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlJSgr5oPkHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And we're ready to go. We can run this on an OpenAI gym environment and watch the magic happen!"
      ]
    },
    {
      "metadata": {
        "id": "vUO1aQ_xo8RM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with tf.Session() as session:\n",
        "        env = gym.make('MountainCar-v0')\n",
        "\n",
        "        np.random.seed(1728)\n",
        "        tf.set_random_seed(1728)\n",
        "\n",
        "        state_dims = env.observation_space.shape[0]\n",
        "        actions_num = env.action_space.n\n",
        "        learning_rate = 0.0001\n",
        "        tau = 0.001\n",
        "\n",
        "        actor = Actor(state_dims, actions_num, learning_rate, tau)\n",
        "\n",
        "        print(\"Training beginning\")\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        print(tf.global_variables())\n",
        "        train(session, env, actor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFKbMVTuo-Sn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13501
        },
        "outputId": "856eef4d-17c0-47eb-c313-bd8a67c95121"
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training beginning\n",
            "[<tf.Variable 'batch_normalization/gamma:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/beta:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/moving_mean:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/moving_variance:0' shape=(2,) dtype=float32>, <tf.Variable 'dense/kernel:0' shape=(2, 64) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/gamma:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/moving_mean:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/moving_variance:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(64, 3) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(3,) dtype=float32>, <tf.Variable 'batch_normalization_2/gamma:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/beta:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/moving_mean:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/moving_variance:0' shape=(2,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(2, 64) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/gamma:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/beta:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/moving_mean:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/moving_variance:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_3/kernel:0' shape=(64, 3) dtype=float32>, <tf.Variable 'dense_3/bias:0' shape=(3,) dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'batch_normalization/gamma/Adam:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/gamma/Adam_1:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/beta/Adam:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/beta/Adam_1:0' shape=(2,) dtype=float32>, <tf.Variable 'dense/kernel/Adam:0' shape=(2, 64) dtype=float32>, <tf.Variable 'dense/kernel/Adam_1:0' shape=(2, 64) dtype=float32>, <tf.Variable 'dense/bias/Adam:0' shape=(64,) dtype=float32>, <tf.Variable 'dense/bias/Adam_1:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/gamma/Adam:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/gamma/Adam_1:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/beta/Adam:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_1/beta/Adam_1:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_1/kernel/Adam:0' shape=(64, 3) dtype=float32>, <tf.Variable 'dense_1/kernel/Adam_1:0' shape=(64, 3) dtype=float32>, <tf.Variable 'dense_1/bias/Adam:0' shape=(3,) dtype=float32>, <tf.Variable 'dense_1/bias/Adam_1:0' shape=(3,) dtype=float32>, <tf.Variable 'batch_normalization_2/gamma/Adam:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/gamma/Adam_1:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/beta/Adam:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/beta/Adam_1:0' shape=(2,) dtype=float32>, <tf.Variable 'dense_2/kernel/Adam:0' shape=(2, 64) dtype=float32>, <tf.Variable 'dense_2/kernel/Adam_1:0' shape=(2, 64) dtype=float32>, <tf.Variable 'dense_2/bias/Adam:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_2/bias/Adam_1:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/gamma/Adam:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/gamma/Adam_1:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/beta/Adam:0' shape=(64,) dtype=float32>, <tf.Variable 'batch_normalization_3/beta/Adam_1:0' shape=(64,) dtype=float32>, <tf.Variable 'dense_3/kernel/Adam:0' shape=(64, 3) dtype=float32>, <tf.Variable 'dense_3/kernel/Adam_1:0' shape=(64, 3) dtype=float32>, <tf.Variable 'dense_3/bias/Adam:0' shape=(3,) dtype=float32>, <tf.Variable 'dense_3/bias/Adam_1:0' shape=(3,) dtype=float32>]\n",
            "EPISODE  0\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  1\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  2\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  3\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  4\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  5\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  6\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  7\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  8\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  9\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  10\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  11\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  12\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  13\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  14\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  15\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  16\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  17\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  18\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  19\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  20\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  21\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  22\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  23\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  24\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  25\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  26\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  27\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  28\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  29\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  30\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  31\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  32\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  33\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  34\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  35\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  36\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  37\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  38\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  39\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  40\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  41\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  42\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  43\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  44\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  45\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  46\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  47\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  48\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  49\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  50\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  51\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  52\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  53\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  54\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  55\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  56\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  57\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  58\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  59\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  60\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  61\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  62\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  63\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  64\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  65\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  66\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  67\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  68\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  69\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  70\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  71\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  72\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  73\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  74\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  75\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  76\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  77\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  78\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  79\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  80\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  81\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  82\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  83\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  84\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  85\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  86\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  87\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  88\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  89\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  90\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  91\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  92\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  93\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  94\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  95\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  96\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  97\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  98\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  99\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  100\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  101\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  102\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  103\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  104\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  105\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  106\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  107\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  108\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  109\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  110\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  111\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  112\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  113\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  114\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  115\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  116\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  117\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  118\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  119\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  120\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  121\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  122\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  123\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  124\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  125\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  126\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  127\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  128\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  129\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  130\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  131\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  132\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  133\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  134\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  135\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  136\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  137\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  138\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  139\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  140\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  141\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  142\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  143\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  144\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  145\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  146\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  147\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  148\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  149\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  150\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  151\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  152\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  153\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  154\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  155\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  156\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  157\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  158\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  159\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  160\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  161\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  162\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  163\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  164\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  165\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  166\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  167\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  168\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  169\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  170\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  171\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  172\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  173\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  174\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  175\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  176\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  177\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  178\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  179\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  180\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  181\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  182\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  183\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  184\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  185\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  186\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  187\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  188\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  189\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  190\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  191\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  192\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  193\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  194\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  195\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  196\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  197\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  198\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  199\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  200\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  201\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  202\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  203\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  204\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  205\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  206\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  207\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  208\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  209\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  210\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  211\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  212\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  213\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  214\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  215\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  216\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  217\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  218\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  219\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  220\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  221\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  222\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  223\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  224\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  225\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  226\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  227\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  228\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  229\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  230\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  231\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  232\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  233\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  234\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  235\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  236\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  237\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  238\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  239\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  240\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  241\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  242\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  243\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  244\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  245\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  246\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  247\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  248\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  249\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  250\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  251\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  252\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  253\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  254\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  255\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  256\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  257\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  258\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  259\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  260\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  261\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jL6gUcznCq-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bummer!\n",
        "\n",
        "Okay, great. So we ran it overnight and it never managed to get out of the ditch. I found a great article [here](https://medium.com/ml-everything/reinforcement-learning-with-sparse-rewards-8f15b71d18bf) that does a great job of describing this phenomenon. We got all the math just right, and yet the learner just won't do anything interesting.\n",
        "\n",
        "Well, I don't know what to do about this yet, if I'm being honest with you. Stay tuned!"
      ]
    }
  ]
}