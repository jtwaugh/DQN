{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtwaugh/DQN/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "mAnUCp6bQC7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Q Learning: The Math\n",
        "\n",
        "Let's build a deep Q network! For a recent interview, I spent three days bootstrapping one of these to prototype a learn-to-rank system, and it felt worthwhile to revisit this and really nail down the details. I've also been meaning to give myself a real project to teach myself TensorFlow, and this task seemed as good as any. In this first bit, I build a deep Q learner following the mathematical specifications, and I run it in an OpenAI gym environment."
      ]
    },
    {
      "metadata": {
        "id": "ek9tQXYKp75X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is a Deep Q Network?\n",
        "\n",
        "Great question. Let's wow you with a diagram first, because it really feels like there ought to be an immediate visual for a complicated object:\n",
        "\n",
        "![alt text](https://imgur.com/rrXj8vI.png)\n",
        "\n",
        "Cool. The intuitive explanation is pretty simple. We have an actor who interacts with an environment, which has a state. Acting on the environment changes its state and elicits a certain reward. The goal is to maximize reward by modifying behavior. In Q learning, the program keeps track of its history, and the actor accesses the history in order to train on past events. At every step, the actor berates himself for past actions, estimating in retrospect how much reward he thinks he could have elicited in comparison to what he got, and readjusts. I find this a relatable human phenomenon, and I suspect you do, as well. \n",
        "\n",
        "In order to do this learning, the actor needs to keep track of both his present method and the evolving method that updates through the training process. Imagine trying to learn a new skill and being told by your mentor to \"do that again the way you were doing it, so we can see what you'e doing wrong.\" Same deal for the actor: in order to avoid comparing an updating function against itself, we need a network to hold a frozen copy of the actor's parameters during training.\n",
        "\n",
        "Anyway, I've separated the arrows into four types. The gold arrows indicate encapsulated behavior. In the networks, this is just TensorFlow doing neural net stuff; in the environment, this is totally unseen by the program written here and we won't pretend to know anything about it. The solid arrows represent the flow of information during the interaction process: the actor sees a state, acts, and it changes the environment and produces reward. The dashed arrows are simply the recording of historical data. The dotted arrows represent the flow of information during the learning process: the actor replays the history, computing best possible actions according to the frozen estimator, and adjusting the new parameters to gradient-descend toward the maxima from the frozen ones. Finally, the frozen parameters have to slowly update toward the new ones to keep things moving."
      ]
    },
    {
      "metadata": {
        "id": "ugBCvYs2ZjmT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Okay, But I Really Just Wanted to Know Why It's Called a 'Q' Network...\n",
        "\n",
        "Sure. If you've done any advanced math, these sorts of names come up all the time (algebraic K theory, for instance), and you ought to know better than to ask. But if you're like me, then you still want to ask, and this time there actually is a good answer: Q is the name of the hidden function that returns the maximum reward for each action at each state, and the actor is trying to lern it. \n",
        "\n",
        "Let's use this as a segue into some notation.\n",
        "\n",
        "Because we're doing reinforcement learning, our session consists of an \"environment\" object (in this case, the OpenAI gym) and an \"actor\" object represented by a neural network, and the actor will choose actions based on the observed state of the environment weighted by the speculative reward that they will produce. The map from states to actions we call the actor's *policy*. Because the network is parameterized, we can use stochastic gradient descent to try to find an optimal policy for the environment, which we take to be a satisfactory solution to the problem.\n",
        "\n",
        "When we speak of value, of course, it's important to remember that we are in fact trying to choose a policy that creates the most *long-term* value, i.e. not naive greedy value. This matters for understanding deep Q learning. Let $R_t : s_t, a_t \\mapsto r_t$ denote the reward function of the environment at time $t$. As I mentioned, we assume no conditions on the behavior of $R$. Additionally, let $\\pi: s_t \\mapsto a_t$ denote the actor's policy. Because it's parameterized by a neural net, we can speak of a maximum over functions indexed over it. Finally, let $\\gamma \\in [0..1]$ be the *discount rate*, which determines the trade-off between present and future value. Now recall the Bellman equation from the machine learning literature:\n",
        "\n",
        "$$ Q(s_0) = \\max_{\\pi} \\sum_{t = 0}^{\\infty} \\gamma^t r_t$$\n",
        "\n",
        "This states that the desirability of the environment being in some state $s_0$ is the maximum long-term reward it could yield under any policy. By \"long-term reward\" we mean the sum of reward values (remember $r_t = R_t(s_t, a_t)$ and $s_t$ is a (possibly nondeterministic) function of $a_{t-1}$ and $s_{t-1}$, and $a_t = \\pi(s_{t-1})$.\n",
        "\n",
        "Some pseudocode is probably instructive at this point. "
      ]
    },
    {
      "metadata": {
        "id": "qUf_iSLdqU70",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General sketch of a policy iteration\n",
        "# Additional code will try to improve the function actor.policy()\n",
        "def pseudo_policy_iterate(env, actor, maxTime):\n",
        "  actor.initialize()\n",
        "  env.initialize()\n",
        "  # Get s_0\n",
        "  state = env.get_initial_state()\n",
        "  \n",
        "  for t in range(maxTime):\n",
        "    # a_t = \\pi(s_{t-1})\n",
        "    action = actor.policy(state)\n",
        "    # r_t = R_t(s, a)\n",
        "    reward = env.get_reward(t, state, action)\n",
        "    # a_{t-1}, s-{t-1} ->  s_t\n",
        "    new_state = env.reaction(state, action)\n",
        "    state = new_state\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_2MrsOAMr-HI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If the function $Q$ is known completely, and if the state resulting from each action is deterministically known, then a greedy algorithm directly yields the optimal policy, i.e. the information about the optimal policy is already in the function. However, we know neither the full reward function nor do we know how to reach one state from another by means of a certain action. As it turns out, a greedy algorithm will remain optimal in most practical cases, but this is merely an empirical fact. In light of this, we really want to investigate the following:\n",
        "\n",
        "$$ Q_{\\pi}(s, a) = \\mathbb{E}\\left[ \\sum_{t = 0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a, a_t = \\pi(s_{t-1}) \\right] $$\n",
        "\n",
        "That is, we suppose we have partial information about the best policy given an action on a certain present state and we set our actor to work trying to estimate a policy that approaches the value $Q_*(s, a) = \\max_{\\pi} Q_{\\pi}(s, a)$ at each state $s$ and each action $a$.\n",
        "\n",
        "There you have it: it's a deep neural net that's trying to efficiently approximate $Q_*$ in order to determine an optimal policy in a given environment."
      ]
    },
    {
      "metadata": {
        "id": "PTwKd7W3ATOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Loss Function with Respect to Historical Data\n",
        "\n",
        "Great, so how do we define a loss function that gets us there? Obviously there's no guarantee of a global maximum, so we have to choose some sort of compromise that will converge us to a good estimate with reasonable resources. \n",
        "\n",
        "What's distinctive about deep Q learning is that the actor \"introspects\" after every action: the program keeps a \"subject history\" of the actor, which will get minibatch-sampled at every step to readjust the actor network against a new estimate of the maximum-value policy. The idea here is to continually re-estimate our long-term rewards given new information from the environment. That is, we want to figure out the best the actor \"could have done\" given all of its present information, then compare the current policy to the estimated ideal performance. Let $\\hat{Q}(s, a, \\theta)$ denote the actor network's estimate of $Q(s, a)$ at parameters $\\theta$. We define the *time-difference (TD) error* as follows:\n",
        "\n",
        "$$\\delta(t) = | r_t + \\gamma \\cdot \\max_{a_{t+1}} \\hat{Q}(s_{t+1}, a_{t+1}, \\theta^{-})  - \\hat{Q}(s, a, \\theta)  | $$\n",
        "\n",
        "Here we denote by $\\theta^{-}$ the set of parameters the neural net takes at the beginning of the experience-replay process, whereas $\\theta$ is allowed to update continuously over a replay-training session. Again, we'll pseudocode this:"
      ]
    },
    {
      "metadata": {
        "id": "_fEAqU3ss1NW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pseudo_replay_learn(actor, history, gamma):\n",
        "  # Freeze our estimator parameters\n",
        "  target_params = copy(actor.parameters)\n",
        "  # Learn over many iterations\n",
        "  for epsiode in range(max_episodes):\n",
        "    loss = 0.0\n",
        "    for event in history:\n",
        "      (state, action, reward, time, newState) = event\n",
        "      # Compute \\hat{Q}(s_{t+1}, a_{t+1}, \\theta^-)\n",
        "      action_values = [ actor.estimate(newState, newAction, target_params)\\\n",
        "                        for newAction in actor.actions ]\n",
        "      # Get our new estimate\n",
        "      new_estimate = actor.estimate(state, action, actor.parameters)\n",
        "      # Get our TD error\n",
        "      delta = (reward + gamma * max(action_values) - new_estimate)\n",
        "      # Update our MSE\n",
        "      loss += delta**2\n",
        "    # Adjust our estimate weights each episode\n",
        "    actor.update_weights(error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nEbrpgT0PUZy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Environment and Exploration/Exploitation\n",
        "\n",
        "Let's put this together, with one final concern.\n",
        "\n",
        "Notice that below I include a chance of acting randomly: this is called an $\\varepsilon$*-greedy algorithm* and it's an attempted answer to the problem of *exploration-exploitation* tradeoff. This is a pretty easy concern to visualize, especially in light of the performance this code initially elicits. \n",
        "\n",
        "Here's the MountainCar environment that I deploy the network in:\n",
        "\n",
        "![alt text](http://gym.openai.com/v2018-02-21/videos/MountainCar-v0-270f34b9-f23e-4d95-a933-4c902b4f4435/poster.jpg)\n",
        "\n",
        "The environment simulates Newtonian mechanics in two dimensions, and you can see from your own physical intuition how the actor can win. The reward is -1.0 for every frame at which the cart is not sitting at the flag, the the environment terminates as soon as the cart touches the flag. Therefore, the optimal policy gets the cart to the top as quickly as possible.\n",
        "\n",
        "How will the deep Q learner find this policy? Imagine we've been sitting for one hundred frames and trying to learn on past experiences. Having not yet seen a successful policy, the learner will not be maximiing toward anything meaningful. In fact, naively learning like this prevents us from ever succeeding. What's to be done?\n",
        "\n",
        "There's a problem called the [multi-armed bandit](https://en.wikipedia.org/wiki/Multi-armed_bandit) and it lies in that class of problems with stupid-sounding names that you just can't aviod hearing mention of when you study the field to which they pertain. It describes essentially the predicament we're about to get into: it's clear that the actor needs to behave randomly sometimes in order to have anything other than past experience to learn on. In other words, in order to avoid local minima of $\\hat{Q}_{\\theta}$ over the parameter space, we have to *explore* it a bit. However, exploring too much will make the actor unable to actually pursue a single policy and at worst give rise to random motion with no learning. That is, it does need to keep *exploiting* the value it finds through exploration.\n",
        "\n",
        "How do we trade off? Well, the solution to the multi-armed bandit problem, which I am appropriating here without really thinking about it, is the $\\varepsilon$-greedy method. Every step, with probability $p = (1 - \\varepsilon) + \\varepsilon / |A|$ for $A$ the set of possible actions, the actor follows his intuition. Otherwise, he performs a random action.\n",
        "\n",
        "This will hopefully be good enough for now, as it promises that the actor is at least not guaranteed to get stuck."
      ]
    },
    {
      "metadata": {
        "id": "HpVynI00xUxu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General sketch of a policy iteration\n",
        "# Additional code will try to improve the function actor.policy()\n",
        "def pseudo_train(env, actor, gamma, max_time):\n",
        "  # Start the history\n",
        "  history = []\n",
        "  # Start the black boxes\n",
        "  actor.initialize()\n",
        "  env.initialize()\n",
        "  # Get s_0\n",
        "  state = env.get_initial_state()\n",
        "  \n",
        "  for t in range(max_time):\n",
        "    # a_t = \\pi(s_{t-1}) with Pr = (1 - \\epsilon) + (\\epsilon/|A|)\n",
        "    action = -1\n",
        "    if random.random() > (1 - epsilon) + (epsilon / actor.actions_num):\n",
        "        action = random_action()\n",
        "    else:\n",
        "        action = actor.policy(state)\n",
        "    # r_t = R_t(s, a)\n",
        "    reward = env.get_reward(t, state, action)\n",
        "    # a_{t-1}, s-{t-1} ->  s_t\n",
        "    new_state = env.reaction(state, action)\n",
        "    # Memorize\n",
        "    history.append((state, action, reward, time, newState))\n",
        "    # Replay the memory bank and learn\n",
        "    pseudo_replay_learn(env, actor, gamma)\n",
        "    # Advance in time\n",
        "    state = new_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wRtiHwwfxx7F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Of course, we have to tune a number of hyperparameters here. And, of course, in reality we're not going to read the entire history every time but rather minibatch-sample it. And we probably only want to train every several time steps. And we want a max size on the history. But the effect is the same: we're trying to efficiently estimate at each step the total divergence from the best forseeable performance at that step, and then backpropagate the actor network. \n",
        "\n",
        "Following the seminal paper by Mnih et al. (2015), we can write the minibatch loss function, letting $U(D)$ denote a uniform distribution over the history as\n",
        "\n",
        "$$ \\mathcal{L}_i(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\ \\sim \\ U(D)} \\left[ \\left( r + \\max_{a'} \\hat{Q}(s', a', \\theta_i^-) - \\hat{Q}(s, a, \\theta_i) \\right)^2 \\right] $$\n",
        "\n",
        "Now that the main routine is clear enough that we're worrying about implementation concerns like the above, let's actually take a look at the working Python code."
      ]
    },
    {
      "metadata": {
        "id": "YuW9Lzp3i3xK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Q Learning: The Code\n",
        "\n",
        "Okay, now bear with me through some setup stuff on the Google machine."
      ]
    },
    {
      "metadata": {
        "id": "WuA1LTrwYsd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "ed6c74bc-c88d-4f3e-ab77-57e821428747"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip3 install gym\n",
        "!pip3 install Box2D\n",
        "!pip3 install box2d box2d-kengz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.8)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (0.19.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: Box2D in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.6/dist-packages (2.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "od94BwgaZZ8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And the relevant imports:"
      ]
    },
    {
      "metadata": {
        "id": "DedwQDZaY0Hn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3waZSRhekfwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's set a print flag so we can debug:"
      ]
    },
    {
      "metadata": {
        "id": "mXuv_JMjEzlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "debug_print = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZaJ_pteekiS9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And here's the class that implements the \"History\" object. Notice that I use a double-ended queue for efficiency, and that it helps to automatically handle the tupling and untupling of stored events. The function ``sample()`` returns numpy arrays ready for TensorFlow to use."
      ]
    },
    {
      "metadata": {
        "id": "nrtdLywp0UsS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Stores tuples of (state, action, reward, time, new_state)\n",
        "class Buffer:\n",
        "\n",
        "    def __init__(self, cap=1000, seed=1728):\n",
        "        random.seed(seed)\n",
        "        self.buffer = deque()\n",
        "        self.count = 0\n",
        "        self.cap = cap\n",
        "\n",
        "    def append(self, s, a, r, t, s2):\n",
        "        unit = (s, a, r, t, s2)\n",
        "        # Make sure to respect the max capacity\n",
        "        if self.count < self.cap: \n",
        "            self.buffer.append(unit)\n",
        "            self.count += 1\n",
        "        else:\n",
        "            self.buffer.popleft()\n",
        "            self.buffer.append(unit)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Get events\n",
        "        batch = random.sample(self.buffer, min([self.count, batch_size]))\n",
        "        # Print if flagged\n",
        "        if debug_print:\n",
        "            print(\"Sampled \", batch_size, \" events from the buffer.\")\n",
        "        # Now collate the events\n",
        "        # Concatenate here because they're shape (1, state_dims)\n",
        "        s_batch = np.concatenate([s for (s, a, r, t, s2) in batch])\n",
        "        # Reshape here because we need a second dimension\n",
        "        a_batch = np.reshape([a for (s, a, r, t, s2) in batch], (batch_size, 1))\n",
        "        r_batch = np.reshape([r for (s, a, r, t, s2) in batch], (batch_size, 1))\n",
        "        t_batch = np.reshape([t for (s, a, r, t, s2) in batch], (batch_size, 1))\n",
        "        # Concatenate here because they're shape (1, state_dims)\n",
        "        s2_batch = np.concatenate([s2 for (s, a, r, t, s2) in batch])\n",
        "        return s_batch, a_batch, r_batch, t_batch, s2_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oyMStkAX00xX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll build an actual deep Q network. As alluded to above, when we freeze the parameters $\\theta^-$, we're better off just keeping an entire other network, christened the *target network*. To do this, then, we include a function to build these both. \n",
        "\n",
        "Additionally, we're going to have to custom-define the loss function with its own computation graph, since (to my knowledge) TensorFlow does not include a built-in Q-learning module. (It's also a good exercise.) This is part of the beauty of TensorFlow. Here's a sketch I made of what's going on. The gold boxes represent placeholder Tensor objects with no defined size, whereas the grey boxes represent batches of size ``batch_size``. Considering only the graph on the gold boxes allows us to define a \"flow\" for the tensors that's represented totally abstractly. Nowhere in defining the loss function do I call ``session.run()``, because the function is just a graph.\n",
        "\n",
        "![alt text](https://imgur.com/FmSDJAH.png)\n",
        " \n",
        "We can see the flow between the two networks, although the actual mathematics is now obscured. When we use the object ``self.loss``, the whole depicted apparatus of circular nodes and their edges is what we're working with. It was working through how to set this up that finally made the meaning of the term \"TensorFlow\" finally click for me.\n",
        "\n",
        "In fact, this is the only effective way to do this. If we remember how backpropagation works, the only way to actually learn is to adjust each parameter in each network according to the loss function's gradient with respect to that parameter. The only way that we can get TensorFlow to compute all those gradients for us is to define the loss function as a graph. That is, had we not done it this way, the loss function wouldn't be TensorFlow-differentiable and we'd be hosed. However, this way, training is now straightforward. Just take the optimizer that we're accustomed to using and tell it to minimize the custom loss function. I define a ``train`` function as a wrapper, but as you can see, all it does is run the provided session on the loss function mimizer."
      ]
    },
    {
      "metadata": {
        "id": "lZhAHyev1JbZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Actor:\n",
        "\n",
        "    def __init__(self, state_dims, actions_num, hidden_size,\\\n",
        "                 learning_rate, tau):\n",
        "        # Network parameters\n",
        "        self.state_dims = state_dims\n",
        "        self.actions_num = actions_num\n",
        "        self.hidden_size = hidden_size\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = learning_rate\n",
        "        # Learning rate for target network\n",
        "        self.tau = tau\n",
        "\n",
        "        # Assemble the actor\n",
        "        self.actor_input, self.actor_network = self.build_nn()\n",
        "        self.actor_params = tf.trainable_variables()\n",
        "        # Might want to normalize these\n",
        "        self.actor_gradients = tf.gradients(self.actor_network,\\\n",
        "                                            self.actor_params)\n",
        "        # Get the optimizer involved\n",
        "        self.opt = tf.train.AdamOptimizer(self.learning_rate)\n",
        "\n",
        "        # Assemble the target network - this keeps values constant in training\n",
        "        self.target_input, self.target_network = self.build_nn()\n",
        "        self.target_params = tf.trainable_variables()[len(self.actor_params):]\n",
        "        # We might want to normalize these\n",
        "        self.target_gradients = tf.gradients(self.target_network,\\\n",
        "                                             self.target_params)\n",
        "\n",
        "        # Have the target parameters slowly catch up to the actor parameters\n",
        "        self.update_target = [ self.target_params[i].assign(\\\n",
        "            tf.multiply(self.actor_params[i], self.tau) +\n",
        "            tf.multiply(self.target_params[i], 1. - self.tau))\\\n",
        "            for i in range(len(self.target_params)) ]\n",
        "\n",
        "        # Placeholders for the batch values\n",
        "        self.loss_actions = tf.placeholder(tf.int32, shape=(None, 1))\n",
        "        self.loss_rewards = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "        \n",
        "        # Build the graph for the loss function\n",
        "        target_rewards = self.loss_rewards + \\\n",
        "                          tf.reduce_max(self.target_network, axis=1)\n",
        "        estimates = tf.batch_gather(self.actor_network, self.loss_actions)\n",
        "        td_error = target_rewards - estimates\n",
        "        self.loss = tf.reduce_mean(tf.square(td_error))\n",
        "        \n",
        "        # Operation for updating the actor network's weights\n",
        "        self.update_actor = self.opt.minimize(self.loss,\\\n",
        "                                              var_list=self.actor_params)\n",
        "        \n",
        "    # Encapsulate this because we're going to call it twice\n",
        "    def build_nn(self):\n",
        "        inputLayer = tf.placeholder(tf.float32, shape=(None, self.state_dims))\n",
        "        hiddenLayer = Dense(self.hidden_size)\n",
        "        # The network is trying to estimate the value for each possible action\n",
        "        outputLayer = Dense(self.actions_num)\n",
        "        # And put them all together\n",
        "        layers = BatchNormalization()(inputLayer)\n",
        "        layers = Activation('relu')(layers)\n",
        "        layers = hiddenLayer(layers)\n",
        "        layers = BatchNormalization()(layers)\n",
        "        layers = Activation('relu')(layers)\n",
        "        layers = outputLayer(layers)\n",
        "        layers = Activation('softmax')(layers)\n",
        "        # We have to return the input layer to give it data\n",
        "        return inputLayer, layers\n",
        "       \n",
        "    # Predict on a numpy array\n",
        "    # Should return an integer for the action number\n",
        "    def predict(self, session, inputs):\n",
        "        return session.run(self.actor_network,\\\n",
        "                           feed_dict={ self.actor_input : inputs } )\n",
        "\n",
        "    # Push the minibatch to the loss function minimizer\n",
        "    def train(self, session, s_batch, a_batch, r_batch, s2_batch):\n",
        "        session.run( self.update_actor,\\\n",
        "                     feed_dict = { self.actor_input : s_batch,\\\n",
        "                                   self.loss_actions : a_batch,\\\n",
        "                                   self.loss_rewards : r_batch,\\\n",
        "                                   self.target_input : s2_batch } )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "630AgkoTlEdl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There's a lot going on up there. The ``build_nn`` function just puts together a deep net with one hidden layer, and it has to return both the input and output layers so that we can pipe in the batches from the history buffer. We have to keep track of the actor and target parameters so that we can update the target parameters slowly."
      ]
    },
    {
      "metadata": {
        "id": "Y0KKuwr7o3Ip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training routine within a TF session\n",
        "# TODO log the reward values for TensorBoard\n",
        "def train(session, env, actor,\\\n",
        "          epsilon=0.10, ep_num=1000, ep_length=1000, minibatch_size=64):\n",
        "    env.reset()\n",
        "    # Keep the buffer here\n",
        "    history = Buffer()\n",
        "    # Update target network parameters\n",
        "    session.run(actor.update_target)\n",
        "    # Start running episodes\n",
        "    for ep in range(ep_num):\n",
        "        # Print\n",
        "        print(\"EPISODE \", ep)\n",
        "        # Use matplotlib to render in Colab\n",
        "        #plt.imshow(env.render(mode='rgb_array'))\n",
        "        # Per episode, reset the environment\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        ep_avg_max = 0\n",
        "        # Run through the epochs\n",
        "        for epoch in range(ep_length):\n",
        "            \n",
        "            # Get the most favored action given the actor's prediction\n",
        "            state_vec = np.reshape(state, (1, actor.state_dims))\n",
        "            \n",
        "            # Get the action as the index in the distribution\n",
        "            # But we want to do this epsilon-greedily or else get stuck\n",
        "            action = -1\n",
        "            if random.random() > (1 - epsilon) + (epsilon / actor.actions_num):\n",
        "                action = random.randint(0, actor.actions_num - 1)\n",
        "            else:\n",
        "                action = np.argmax(actor.predict(session, state_vec))\n",
        "            \n",
        "            # Print if flagged\n",
        "            if debug_print:\n",
        "                print(\"Performed action \", action)\n",
        "            \n",
        "            # Feed this into the gym environment\n",
        "            new_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            # Print if flagged\n",
        "            if debug_print:\n",
        "                print(\"Got reward \", reward)\n",
        "                print(\"Resulted in state \", new_state)\n",
        "            \n",
        "            # If the actor won or died or whatever it is, break the loop\n",
        "            if done:\n",
        "                # Print if flagged\n",
        "                if debug_print:\n",
        "                    print(\"Terminated.\")\n",
        "                break\n",
        "                \n",
        "            # Store the results as a tuple\n",
        "            history.append( state_vec, action, reward, done, \\\n",
        "                            np.reshape(new_state, (1, actor.state_dims)) )\n",
        "            \n",
        "            # If we're large enough, take a minibatch sample and train the net\n",
        "            if history.count > minibatch_size:\n",
        "                # Get the arrays corresponding to the values\n",
        "                s_batch, a_batch, r_batch, t_batch, s2_batch =\\\n",
        "                 history.sample(minibatch_size)\n",
        "                # Feed this to the DQN to train\n",
        "                actor.train(session, s_batch, a_batch, r_batch, s2_batch)\n",
        "            \n",
        "            # Update the state\n",
        "            state = new_state\n",
        "            # Tabulate reward\n",
        "            ep_reward += reward\n",
        "            # The paper claims we do this in here\n",
        "            # Let the target parameters crawl toward the actor parameters\n",
        "            session.run(actor.update_target)\n",
        "        # Print\n",
        "        print(\"Total reward: \", ep_reward, \"\\n-----------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qlJSgr5oPkHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And we're ready to go. We can run this on an OpenAI gym environment and watch the magic happen!"
      ]
    },
    {
      "metadata": {
        "id": "vUO1aQ_xo8RM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    with tf.Session() as session:\n",
        "        env = gym.make('MountainCar-v0')\n",
        "\n",
        "        np.random.seed(1)\n",
        "        tf.set_random_seed(1)\n",
        "\n",
        "        state_dims = env.observation_space.shape[0]\n",
        "        actions_num = env.action_space.n\n",
        "        hidden_size = 32\n",
        "        learning_rate = 0.001\n",
        "        tau = 0.1\n",
        "\n",
        "        actor = Actor(state_dims, actions_num, hidden_size, learning_rate, tau)\n",
        "\n",
        "        print(\"Training beginning\")\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        print(tf.global_variables())\n",
        "        train(session, env, actor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFKbMVTuo-Sn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1747
        },
        "outputId": "77186e0b-f728-4b99-9655-379f5099b0bd"
      },
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training beginning\n",
            "[<tf.Variable 'batch_normalization/gamma:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/beta:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/moving_mean:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/moving_variance:0' shape=(2,) dtype=float32>, <tf.Variable 'dense/kernel:0' shape=(2, 32) dtype=float32>, <tf.Variable 'dense/bias:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/moving_mean:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/moving_variance:0' shape=(32,) dtype=float32>, <tf.Variable 'dense_1/kernel:0' shape=(32, 3) dtype=float32>, <tf.Variable 'dense_1/bias:0' shape=(3,) dtype=float32>, <tf.Variable 'batch_normalization_2/gamma:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/beta:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/moving_mean:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization_2/moving_variance:0' shape=(2,) dtype=float32>, <tf.Variable 'dense_2/kernel:0' shape=(2, 32) dtype=float32>, <tf.Variable 'dense_2/bias:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_3/gamma:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_3/beta:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_3/moving_mean:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_3/moving_variance:0' shape=(32,) dtype=float32>, <tf.Variable 'dense_3/kernel:0' shape=(32, 3) dtype=float32>, <tf.Variable 'dense_3/bias:0' shape=(3,) dtype=float32>, <tf.Variable 'beta1_power:0' shape=() dtype=float32_ref>, <tf.Variable 'beta2_power:0' shape=() dtype=float32_ref>, <tf.Variable 'batch_normalization/gamma/Adam:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/gamma/Adam_1:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/beta/Adam:0' shape=(2,) dtype=float32>, <tf.Variable 'batch_normalization/beta/Adam_1:0' shape=(2,) dtype=float32>, <tf.Variable 'dense/kernel/Adam:0' shape=(2, 32) dtype=float32>, <tf.Variable 'dense/kernel/Adam_1:0' shape=(2, 32) dtype=float32>, <tf.Variable 'dense/bias/Adam:0' shape=(32,) dtype=float32>, <tf.Variable 'dense/bias/Adam_1:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/gamma/Adam:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/gamma/Adam_1:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/beta/Adam:0' shape=(32,) dtype=float32>, <tf.Variable 'batch_normalization_1/beta/Adam_1:0' shape=(32,) dtype=float32>, <tf.Variable 'dense_1/kernel/Adam:0' shape=(32, 3) dtype=float32>, <tf.Variable 'dense_1/kernel/Adam_1:0' shape=(32, 3) dtype=float32>, <tf.Variable 'dense_1/bias/Adam:0' shape=(3,) dtype=float32>, <tf.Variable 'dense_1/bias/Adam_1:0' shape=(3,) dtype=float32>]\n",
            "EPISODE  0\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  1\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  2\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  3\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  4\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  5\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  6\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  7\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  8\n",
            "Total reward:  -199.0 \n",
            "-----------\n",
            "EPISODE  9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-ab4ce84bdb89>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e1e1a76038e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(session, env, actor, epsilon, ep_num, ep_length, minibatch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m \u001b[0;34m=\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# Feed this to the DQN to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Update the state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-fd4625c58f3d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, session, s_batch, a_batch, r_batch, s2_batch)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# Push the minibatch to the loss function minimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_actor\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_input\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_actions\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_rewards\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_input\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0ms2_batch\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "jL6gUcznCq-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bummer!\n",
        "\n",
        "Okay, great. So we ran it overnight and it never managed to get out of the ditch. I found a great article [here](https://medium.com/ml-everything/reinforcement-learning-with-sparse-rewards-8f15b71d18bf) that does a great job of describing this phenomenon. We got all the math just right, and yet the learner just won't do anything interesting.\n",
        "\n",
        "Well, one big thing is that I initially forgot to actually pass the actor's parameters into the SGD optimizer that's supposed to train it. If you don't do this, the parameters never update and you're definitely going to stay stuck. Remember to keep track of your graph!\n",
        "\n",
        "Well, I don't know what to do about this yet, if I'm being honest with you. Stay tuned!"
      ]
    }
  ]
}